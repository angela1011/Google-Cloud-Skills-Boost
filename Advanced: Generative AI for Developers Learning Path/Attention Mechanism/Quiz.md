# Attention Mechanism: Quiz

1. What is the advantage of using the attention mechanism over a traditional recurrent neural network (RNN) encoder-decoder?
```bash
The attention mechanism lets the decoder focus on specific parts of the input sequence, which can improve the accuracy of the translation.
```

2. What is the name of the machine learning architecture that can be used to translate text from one language to another?
```bash
Encoder-decoder
```

3. What is the name of the machine learning technique that allows a neural network to focus on specific parts of an input sequence?
```bash
Attention mechanism
```

4. How does an attention model differ from a traditional model?
```bash
Attention models pass a lot more information to the decoder.
```

5. What is the advantage of using the attention mechanism over a traditional sequence-to-sequence model?
```bash
The attention mechanism lets the model focus on specific parts of the input sequence.
```

6. What is the purpose of the attention weights?
```bash
To assign weights to different parts of the input sequence, with the most important parts receiving the highest weights.
```

7. What are the two main steps of the attention mechanism?
```bash
Calculating the attention weights and generating the context vector
```

## Congratulation!